{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5834c125",
   "metadata": {},
   "source": [
    "# 🧠 Pneumonia Detection from Chest X-Rays (From Scratch)\n",
    "\n",
    "This notebook is part of a hands-on project to build a binary image classifier to detect pneumonia from chest X-ray images.  \n",
    "The aim is to understand and implement the entire pipeline **from scratch**, without using high-level deep learning libraries.\n",
    "\n",
    "**Objective**:  \n",
    "- Load and preprocess X-ray images  \n",
    "- Prepare dataset (train/test split)  \n",
    "- Build a classifier (initially logistic regression) using only NumPy  \n",
    "- Evaluate the results and identify future work\n",
    "\n",
    "📌 This is a learning project inspired by the [Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning) by Andrew Ng.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6dc254fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "import copy\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc83ae5",
   "metadata": {},
   "source": [
    "## 🧼 Image Preprocessing\n",
    "\n",
    "In this section, we:\n",
    "- Load X-ray images from the dataset folders\n",
    "- Resize them to a fixed shape (e.g., 128×128)\n",
    "- Convert images to grayscale (if needed)\n",
    "- Normalize pixel values to the range [0, 1]\n",
    "- Flatten the images for input to the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e4668b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, img_size=(128,128)): # Resizing, normalizing & flatting the images.\n",
    "    img = Image.open(image_path).convert(\"L\") # Making sure all are greyscaled\n",
    "    img = img.resize(img_size) # Resizing\n",
    "    img_array = np.array(img).flatten().astype(np.float32) / 255.0 # Flatting and  Normalizing\n",
    "    \n",
    "    return img_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c65005b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'train'\n",
    "test_dir = 'test'\n",
    "labels = ['PNEUMONIA', 'NORMAL']\n",
    "img_size = (128,128)\n",
    "\n",
    "# --- 2. Corrected Data Loading Function ---\n",
    "def get_data(data_dir):\n",
    "    \"\"\"\n",
    "    Loads, resizes, and NORMALIZES images and their corresponding labels.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    image_labels = []\n",
    "    for label in labels:\n",
    "        path = os.path.join(data_dir, label)\n",
    "        for img_file in os.listdir(path):  # Iterate over all files in the directory\n",
    "            img_path = os.path.join(path, img_file)  # Full path to the image file\n",
    "            try:\n",
    "                img_array = preprocess_image(img_path, img_size)\n",
    "                images.append(img_array)\n",
    "                \n",
    "                # Assign label (0 for PNEUMONIA, 1 for NORMAL)\n",
    "                if label == \"PNEUMONIA\":\n",
    "                    image_labels.append(0)\n",
    "                else:\n",
    "                    image_labels.append(1)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {img_path}: {e}\")\n",
    "    \n",
    "    return np.array(images), np.array(image_labels)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0ed4a7",
   "metadata": {},
   "source": [
    "## 🧪 Train-Test Split\n",
    "\n",
    "We shuffle the dataset and split it into:\n",
    "- **Training set**: 80%\n",
    "- **Test set**: 20%\n",
    "\n",
    "This ensures the model can generalize and be evaluated fairly on unseen images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6bb47773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (16384, 5216) (1, 5216)\n",
      "Test set: (16384, 624) (1, 624)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = get_data(train_dir)\n",
    "X_test, y_test = get_data(test_dir)\n",
    "\n",
    "#fixing there shapes:\n",
    "X_train= X_train.T\n",
    "X_test = X_test.T\n",
    "y_train = y_train.reshape(1,-1)\n",
    "y_test = y_test.reshape(1,-1)\n",
    "\n",
    "\n",
    "print(\"Training set:\", X_train.shape, y_train.shape)\n",
    "print(\"Test set:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9cc6bff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    parameters = {}\n",
    "    L = len(layer_dims) # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "    \n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1])*0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "918a2010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.00011528  0.01581172 -0.01592322  0.00042384]\n",
      " [ 0.014572    0.00436423 -0.00257021  0.00623894]\n",
      " [ 0.01226694  0.00285889 -0.01077597  0.0034427 ]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[ 0.0078564   0.0044265   0.00499304]\n",
      " [-0.00642493 -0.00990345 -0.00617311]]\n",
      "b2 = [[0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "parameters = initialize_parameters_deep([4,3,2])\n",
    "\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3112d01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    # The pre-activation function\n",
    "    Z = np.dot(W,A) + b\n",
    "    # As in the describtion, it is for the backword propagation\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "be61fab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression sigmoid function\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "\n",
    "    s = 1/ (1+np.exp(-z))\n",
    "    return s,z\n",
    "\n",
    "def relu(z):\n",
    "    \"\"\"\n",
    "    Compute the relu of z\n",
    "\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- relu(z)\n",
    "    \"\"\"\n",
    "\n",
    "    s = np.maximum(0, z)\n",
    "    return s,z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "98f7d865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        \n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        \n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "32ee93ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU] for the first (L-1) layers->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- activation value from the output (last) layer\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2        # number of layers in the neural network\n",
    "    \n",
    "    #[LINEAR -> RELU] for the first (L-1) layers. \n",
    "\n",
    "    # The for loop starts at 1 because layer 0 is the input.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation=\"relu\")\n",
    "        caches.append(cache)\n",
    "        \n",
    "    \n",
    "    # LINEAR -> SIGMOID.\n",
    "    AL, cache = A, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation=\"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "          \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1a3101d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function.\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (ex: 0 for PNEUMONIA, 1 for NORMAL), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    cost = -(np.sum(Y * np.log(AL) + (1 - Y) * np.log(1 - AL))) / m\n",
    "    \n",
    "    \n",
    "    cost = np.squeeze(cost)     # To make sure your cost's shape is what we expect \n",
    "                                # (e.g. this turns [[17]] into 17).\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "40908db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = (1 / m) * np.dot(dZ, A_prev.T)\n",
    "    db = (1 / m) * np.sum(dZ, axis=1, keepdims=True) # Sum by rows\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e7cef4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing helper functions (relu_backward and sigmoid backward)\n",
    "\n",
    "def relu_backward(dA, activation_cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single ReLU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    activation_cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    Z = activation_cache\n",
    "    dZ = np.array(dA, copy=True)  # Just converting dz to a correct object.\n",
    "    \n",
    "    # When Z <= 0, set dZ to 0 (derivative of ReLU is 0 for Z <= 0)\n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, activation_cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single Sigmoid unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    activation_cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    Z = activation_cache\n",
    "    \n",
    "    # Compute the Sigmoid value (A) from Z\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    \n",
    "    # Compute the derivative of the Sigmoid function\n",
    "    dZ = dA * A * (1 - A)\n",
    "    \n",
    "    return dZ    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "742bac16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        \n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db= linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        \n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db= linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6d522ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 for PNEUMONIA, 1 for NORMAL)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # Last layer (SIGMOID -> LINEAR) gradients.\n",
    "    current_cache = caches[L-1]\n",
    "    dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dAL, current_cache, activation=\"sigmoid\")\n",
    "    grads[\"dA\" + str(L-1)] = dA_prev_temp\n",
    "    grads[\"dW\" + str(L)] = dW_temp\n",
    "    grads[\"db\" + str(L)] = db_temp\n",
    "    \n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l+1)], current_cache, activation=\"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l+1)] = dW_temp\n",
    "        grads[\"db\" + str(l+1)] = db_temp     \n",
    "        \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4bfa4dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(params, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    params -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    parameters = copy.deepcopy(params)\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]        \n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5d5798e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.02, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (n_x, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 for PNEUMONIA, 1 for NORMAL), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    costs = []       # keep track of cost\n",
    "    \n",
    "    # Parameters initialization.\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU] for (L-1) layers -> LINEAR -> SIGMOID.\n",
    "        AL,caches = L_model_forward(X, parameters)\n",
    "        \n",
    "        # Compute cost.\n",
    "        cost = compute_cost(AL, Y)\n",
    "    \n",
    "        # Backward propagation.\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    " \n",
    "        # Update parameters.   \n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                \n",
    "        # Print the cost every 10 iterations and for the last iteration (because I run localy)\n",
    "        if print_cost and (i % 10 == 0 or i == num_iterations - 1):\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if i % 10 == 0:\n",
    "            costs.append(cost)\n",
    "    \n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d940cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not the best archtecture but its enough for the start.\n",
    "layers_dims = [16384, 128, 64, 32, 1] # 5 layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ccf2997b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.6931772509297659\n",
      "Cost after iteration 10: 0.6818933399561413\n",
      "Cost after iteration 20: 0.6716876903398634\n",
      "Cost after iteration 30: 0.6624568387450022\n",
      "Cost after iteration 40: 0.6541062116312736\n",
      "Cost after iteration 50: 0.6465482228350946\n",
      "Cost after iteration 60: 0.6397043669085696\n",
      "Cost after iteration 70: 0.633504260579661\n",
      "Cost after iteration 80: 0.6278845984313655\n",
      "Cost after iteration 90: 0.6227885528585221\n",
      "Cost after iteration 100: 0.6181650252008011\n",
      "Cost after iteration 110: 0.6139679959262327\n",
      "Cost after iteration 120: 0.6101560476200676\n",
      "Cost after iteration 130: 0.6066919040435974\n",
      "Cost after iteration 140: 0.6035420651726247\n",
      "Cost after iteration 150: 0.6006764110678783\n",
      "Cost after iteration 160: 0.598067850841683\n",
      "Cost after iteration 170: 0.5956920160431304\n",
      "Cost after iteration 180: 0.5935269765967358\n",
      "Cost after iteration 190: 0.5915529798569173\n",
      "Cost after iteration 200: 0.5897522303729082\n",
      "Cost after iteration 210: 0.5881086856458838\n",
      "Cost after iteration 220: 0.5866078728211361\n",
      "Cost after iteration 230: 0.5852367304923094\n",
      "Cost after iteration 240: 0.583983463014034\n",
      "Cost after iteration 250: 0.5828374116583217\n",
      "Cost after iteration 260: 0.5817889382022589\n",
      "Cost after iteration 270: 0.5808293191878514\n",
      "Cost after iteration 280: 0.5799506555498178\n",
      "Cost after iteration 290: 0.5791457915882058\n",
      "Cost after iteration 300: 0.5784082398240172\n",
      "Cost after iteration 310: 0.5777321144265001\n",
      "Cost after iteration 320: 0.5771120719609436\n",
      "Cost after iteration 330: 0.5765432587398837\n",
      "Cost after iteration 340: 0.5760212629910302\n",
      "Cost after iteration 350: 0.5755420736212872\n",
      "Cost after iteration 360: 0.5751020402718091\n",
      "Cost after iteration 370: 0.5746978384616932\n",
      "Cost after iteration 380: 0.5743264400292761\n",
      "Cost after iteration 390: 0.5739850848800974\n",
      "Cost after iteration 400: 0.5736712560520671\n",
      "Cost after iteration 410: 0.5733826568460646\n",
      "Cost after iteration 420: 0.5731171905900256\n",
      "Cost after iteration 430: 0.5728729425645764\n",
      "Cost after iteration 440: 0.5726481631575273\n",
      "Cost after iteration 450: 0.5724412529928061\n",
      "Cost after iteration 460: 0.5722507491369626\n",
      "Cost after iteration 470: 0.5720753126756654\n",
      "Cost after iteration 480: 0.5719137180689342\n",
      "Cost after iteration 490: 0.5717648434949364\n",
      "Cost after iteration 500: 0.5716276609801032\n",
      "Cost after iteration 510: 0.5715012280296063\n",
      "Cost after iteration 520: 0.5713846816911698\n",
      "Cost after iteration 530: 0.571277229681627\n",
      "Cost after iteration 540: 0.571178145091617\n",
      "Cost after iteration 550: 0.5710867614345466\n",
      "Cost after iteration 560: 0.5710024664224516\n",
      "Cost after iteration 570: 0.570924698245816\n",
      "Cost after iteration 580: 0.5708529402437091\n",
      "Cost after iteration 590: 0.5707867181494708\n",
      "Cost after iteration 600: 0.5707255961287596\n",
      "Cost after iteration 610: 0.5706691739781272\n",
      "Cost after iteration 620: 0.5706170834806976\n",
      "Cost after iteration 630: 0.5705689846276946\n",
      "Cost after iteration 640: 0.5705245660884795\n",
      "Cost after iteration 650: 0.5704835410059838\n",
      "Cost after iteration 660: 0.5704456455168565\n",
      "Cost after iteration 670: 0.5704106357326335\n",
      "Cost after iteration 680: 0.5703782878435925\n",
      "Cost after iteration 690: 0.5703483961443143\n",
      "Cost after iteration 700: 0.5703207700609456\n",
      "Cost after iteration 710: 0.5702952351923388\n",
      "Cost after iteration 720: 0.5702716305321427\n",
      "Cost after iteration 730: 0.5702498073916155\n",
      "Cost after iteration 740: 0.570229629022746\n",
      "Cost after iteration 750: 0.5702109680193707\n",
      "Cost after iteration 760: 0.5701937077710001\n",
      "Cost after iteration 770: 0.5701777418386673\n",
      "Cost after iteration 780: 0.5701629710274426\n",
      "Cost after iteration 790: 0.5701493040248576\n",
      "Cost after iteration 800: 0.570136657130449\n",
      "Cost after iteration 810: 0.5701249518927562\n",
      "Cost after iteration 820: 0.5701141162707787\n",
      "Cost after iteration 830: 0.5701040833832323\n",
      "Cost after iteration 840: 0.5700947918564544\n",
      "Cost after iteration 850: 0.5700861858105692\n",
      "Cost after iteration 860: 0.5700782130488867\n",
      "Cost after iteration 870: 0.5700708257464114\n",
      "Cost after iteration 880: 0.5700639796600708\n",
      "Cost after iteration 890: 0.570057633955077\n",
      "Cost after iteration 900: 0.5700517501456681\n",
      "Cost after iteration 910: 0.5700462924839073\n",
      "Cost after iteration 920: 0.5700412291900812\n",
      "Cost after iteration 930: 0.5700365307076284\n",
      "Cost after iteration 940: 0.5700321685077735\n",
      "Cost after iteration 950: 0.5700281169000644\n",
      "Cost after iteration 960: 0.5700243526118873\n",
      "Cost after iteration 970: 0.5700208530378398\n",
      "Cost after iteration 980: 0.5700175990340777\n",
      "Cost after iteration 990: 0.5700145709999778\n",
      "Cost after iteration 1000: 0.5700117506860873\n",
      "Cost after iteration 1010: 0.5700091237356094\n",
      "Cost after iteration 1020: 0.5700066751441892\n",
      "Cost after iteration 1030: 0.5700043905518368\n",
      "Cost after iteration 1040: 0.5700022563642207\n",
      "Cost after iteration 1050: 0.5700002617988272\n",
      "Cost after iteration 1060: 0.5699983956814926\n",
      "Cost after iteration 1070: 0.5699966475251307\n",
      "Cost after iteration 1080: 0.5699950066396139\n",
      "Cost after iteration 1090: 0.5699934606550175\n",
      "Cost after iteration 1100: 0.5699920021530608\n",
      "Cost after iteration 1110: 0.5699906195899459\n",
      "Cost after iteration 1120: 0.5699893052708658\n",
      "Cost after iteration 1130: 0.5699880466062138\n",
      "Cost after iteration 1140: 0.5699868313922332\n",
      "Cost after iteration 1150: 0.569985659709521\n",
      "Cost after iteration 1160: 0.5699845414121817\n",
      "Cost after iteration 1170: 0.5699834793797635\n",
      "Cost after iteration 1180: 0.5699824764343097\n",
      "Cost after iteration 1190: 0.5699815202965136\n",
      "Cost after iteration 1200: 0.5699806314578099\n",
      "Cost after iteration 1210: 0.5699798034927028\n",
      "Cost after iteration 1220: 0.5699790262284377\n",
      "Cost after iteration 1230: 0.5699782919528579\n",
      "Cost after iteration 1240: 0.5699775980651467\n",
      "Cost after iteration 1250: 0.5699769375795339\n",
      "Cost after iteration 1260: 0.5699763059412006\n",
      "Cost after iteration 1270: 0.5699757014225622\n",
      "Cost after iteration 1280: 0.5699751216327734\n",
      "Cost after iteration 1290: 0.5699745630226638\n",
      "Cost after iteration 1300: 0.5699740240131211\n",
      "Cost after iteration 1310: 0.5699735027430004\n",
      "Cost after iteration 1320: 0.5699729986670404\n",
      "Cost after iteration 1330: 0.5699725076525262\n",
      "Cost after iteration 1340: 0.5699720292962962\n",
      "Cost after iteration 1350: 0.5699715629993442\n",
      "Cost after iteration 1360: 0.5699711075675221\n",
      "Cost after iteration 1370: 0.5699706624384786\n",
      "Cost after iteration 1380: 0.5699702260850805\n",
      "Cost after iteration 1390: 0.5699697974770328\n",
      "Cost after iteration 1400: 0.5699693757564326\n",
      "Cost after iteration 1410: 0.5699689599156031\n",
      "Cost after iteration 1420: 0.569968549513392\n",
      "Cost after iteration 1430: 0.5699681437877366\n",
      "Cost after iteration 1440: 0.569967741815272\n",
      "Cost after iteration 1450: 0.5699673434604393\n",
      "Cost after iteration 1460: 0.5699669473610106\n",
      "Cost after iteration 1470: 0.5699665536312084\n",
      "Cost after iteration 1480: 0.56996616072274\n",
      "Cost after iteration 1490: 0.5699657694008597\n",
      "Cost after iteration 1500: 0.5699653783855\n",
      "Cost after iteration 1510: 0.569964988572416\n",
      "Cost after iteration 1520: 0.5699645998962365\n",
      "Cost after iteration 1530: 0.569964212069915\n",
      "Cost after iteration 1540: 0.5699638240596744\n",
      "Cost after iteration 1550: 0.5699634347644162\n",
      "Cost after iteration 1560: 0.5699630441293936\n",
      "Cost after iteration 1570: 0.5699626526796132\n",
      "Cost after iteration 1580: 0.5699622601970397\n",
      "Cost after iteration 1590: 0.5699618657656974\n",
      "Cost after iteration 1600: 0.5699614685718312\n",
      "Cost after iteration 1610: 0.5699610686374722\n",
      "Cost after iteration 1620: 0.56996066484384\n",
      "Cost after iteration 1630: 0.569960256819033\n",
      "Cost after iteration 1640: 0.5699598464327914\n",
      "Cost after iteration 1650: 0.5699594324472971\n",
      "Cost after iteration 1660: 0.569959014204817\n",
      "Cost after iteration 1670: 0.5699585919258296\n",
      "Cost after iteration 1680: 0.5699581670382982\n",
      "Cost after iteration 1690: 0.5699577393127728\n",
      "Cost after iteration 1700: 0.569957308843944\n",
      "Cost after iteration 1710: 0.569956873881122\n",
      "Cost after iteration 1720: 0.5699564356398639\n",
      "Cost after iteration 1730: 0.5699559916812725\n",
      "Cost after iteration 1740: 0.5699555426287892\n",
      "Cost after iteration 1750: 0.5699550877334725\n",
      "Cost after iteration 1760: 0.5699546272698405\n",
      "Cost after iteration 1770: 0.5699541597487274\n",
      "Cost after iteration 1780: 0.5699536883656267\n",
      "Cost after iteration 1790: 0.5699532095038327\n",
      "Cost after iteration 1800: 0.569952721675338\n",
      "Cost after iteration 1810: 0.5699522260307678\n",
      "Cost after iteration 1820: 0.5699517243715767\n",
      "Cost after iteration 1830: 0.5699512169796054\n",
      "Cost after iteration 1840: 0.5699507054925147\n",
      "Cost after iteration 1850: 0.5699501891070069\n",
      "Cost after iteration 1860: 0.5699496681411036\n",
      "Cost after iteration 1870: 0.5699491422028133\n",
      "Cost after iteration 1880: 0.5699486076179142\n",
      "Cost after iteration 1890: 0.5699480674019405\n",
      "Cost after iteration 1900: 0.569947521390759\n",
      "Cost after iteration 1910: 0.5699469688618005\n",
      "Cost after iteration 1920: 0.5699464112336119\n",
      "Cost after iteration 1930: 0.5699458462667327\n",
      "Cost after iteration 1940: 0.56994527272586\n",
      "Cost after iteration 1950: 0.5699446931825366\n",
      "Cost after iteration 1960: 0.5699441085613691\n",
      "Cost after iteration 1970: 0.5699435180900945\n",
      "Cost after iteration 1980: 0.569942921544031\n",
      "Cost after iteration 1990: 0.5699423187805445\n",
      "Cost after iteration 2000: 0.5699417104177575\n",
      "Cost after iteration 2010: 0.569941094421739\n",
      "Cost after iteration 2020: 0.5699404722440196\n",
      "Cost after iteration 2030: 0.5699398423393532\n",
      "Cost after iteration 2040: 0.5699392068024238\n",
      "Cost after iteration 2050: 0.5699385659861189\n",
      "Cost after iteration 2060: 0.5699379182979554\n",
      "Cost after iteration 2070: 0.5699372636356953\n",
      "Cost after iteration 2080: 0.5699366018608136\n",
      "Cost after iteration 2090: 0.5699359327218401\n",
      "Cost after iteration 2100: 0.569935256909501\n",
      "Cost after iteration 2110: 0.5699345738908947\n",
      "Cost after iteration 2120: 0.5699338842535097\n",
      "Cost after iteration 2130: 0.5699331872941726\n",
      "Cost after iteration 2140: 0.569932482477046\n",
      "Cost after iteration 2150: 0.5699317691591186\n",
      "Cost after iteration 2160: 0.5699310480338765\n",
      "Cost after iteration 2170: 0.5699303189731658\n",
      "Cost after iteration 2180: 0.5699295820347346\n",
      "Cost after iteration 2190: 0.5699288373868983\n",
      "Cost after iteration 2200: 0.5699280842625132\n",
      "Cost after iteration 2210: 0.5699273224574558\n",
      "Cost after iteration 2220: 0.5699265510982603\n",
      "Cost after iteration 2230: 0.5699257699218795\n",
      "Cost after iteration 2240: 0.5699249785220849\n",
      "Cost after iteration 2250: 0.5699241769249213\n",
      "Cost after iteration 2260: 0.5699233654022138\n",
      "Cost after iteration 2270: 0.569922542957349\n",
      "Cost after iteration 2280: 0.5699217092243042\n",
      "Cost after iteration 2290: 0.5699208626746732\n",
      "Cost after iteration 2300: 0.5699200045734049\n",
      "Cost after iteration 2310: 0.5699191347747211\n",
      "Cost after iteration 2320: 0.5699182520262036\n",
      "Cost after iteration 2330: 0.5699173555999333\n",
      "Cost after iteration 2340: 0.5699164462816122\n",
      "Cost after iteration 2350: 0.5699155232196456\n",
      "Cost after iteration 2360: 0.5699145850980539\n",
      "Cost after iteration 2370: 0.5699136326412407\n",
      "Cost after iteration 2380: 0.5699126661223267\n",
      "Cost after iteration 2390: 0.5699116844124138\n",
      "Cost after iteration 2400: 0.569910685674274\n",
      "Cost after iteration 2410: 0.5699096711522392\n",
      "Cost after iteration 2420: 0.5699086389116685\n",
      "Cost after iteration 2430: 0.569907588621369\n",
      "Cost after iteration 2440: 0.5699065193278283\n",
      "Cost after iteration 2450: 0.5699054341710221\n",
      "Cost after iteration 2460: 0.5699043317847201\n",
      "Cost after iteration 2470: 0.5699032107031391\n",
      "Cost after iteration 2480: 0.5699020717843569\n",
      "Cost after iteration 2490: 0.5699009133428743\n",
      "Cost after iteration 2499: 0.5698998538933218\n"
     ]
    }
   ],
   "source": [
    "# Only 2000 itr since I run localy XD\n",
    "parameters, costs = L_layer_model(X_train, y_train, layers_dims, num_iterations = 2500, print_cost = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870502c0",
   "metadata": {},
   "source": [
    "## 📊 Current Status & Next Steps\n",
    "\n",
    "- ✅ Preprocessing complete and dataset ready\n",
    "- ⏳ Model training is not yet implemented\n",
    "- ❌ Evaluation metrics (accuracy, confusion matrix) still to come\n",
    "\n",
    "### 🚧 Future Plans\n",
    "- Implement Convolutional neural networks from scratch\n",
    "- Hyperparameters tuning\n",
    "- Evaluate performance with accuracy and confusion matrix\n",
    "- (Optionally) visualize predictions\n",
    "\n",
    "> Due to time constraints, development is paused here temporarily.  \n",
    "> The project will be revisited after completing the Deep Learning Specialization with more advanced implementations.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
